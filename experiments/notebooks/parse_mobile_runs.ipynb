{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from pandas.errors import EmptyDataError\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"iPhone_14_Pro\"\n",
    "# device = \"Galaxy_S23\"\n",
    "# device = \"iPhone_SE\"\n",
    "# device = \"Pixel_6a\"\n",
    "device = \"**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_dirs = []\n",
    "for app in [\"MLCChat\", \"MLCChat++\", \"LLMFarmEval\", \"LlamaCpp\"]:\n",
    "    if app in [\"MLCChat\", \"MLCChat++\"]:\n",
    "        model = \"**/\"\n",
    "    else:\n",
    "        model = \"**/**\"\n",
    "    model_dirs.extend(glob.glob(f\"../../experiment_outputs/{device}/{app}/{model}/run*\"))\n",
    "\n",
    "app = \"**\"\n",
    "model_dirs = sorted(model_dirs)\n",
    "print(f\"{len(model_dirs)} found!\")\n",
    "for model_dir in model_dirs[:]:\n",
    "    print(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats_plots(dir):\n",
    "    for d in dir:\n",
    "        if not os.path.exists(os.path.join(d, \"results_model_inference_measurements.csv\")):\n",
    "            print(d)\n",
    "            !python ../..//src/tools/report-measurements-llms.py -p {d}\n",
    "generate_stats_plots(model_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(glob.glob(f\"../../experiment_outputs/{device}/{app}/**/run*/results_model_inference_measurements.csv\"))\n",
    "files.extend(glob.glob(f\"../../experiment_outputs/{device}/{app}/**/**/run*/results_model_inference_measurements.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for f in files:\n",
    "    application = re.search(\"experiment_outputs/.*?/(.*?)/\", f).group(1)\n",
    "    le_device = re.search(\"experiment_outputs/(.*?)/\", f).group(1)\n",
    "    if application in [\"MLCChat\", \"MLCChat++\"]:\n",
    "        regex = \"run_cs(\\d+)_mgl(\\d+)\"\n",
    "    else:\n",
    "        regex = \"run_cs(\\d+)_mgl(\\d+)_bs(\\d+)\"\n",
    "    model_regex = f\"{re.escape(application)}/(.*)/run\"\n",
    "    match = re.search(regex, f)\n",
    "    if match:\n",
    "        context_size = int(match.group(1))\n",
    "        max_gen_len = int(match.group(2))\n",
    "        if \"MLCChat\" not in application:\n",
    "            batch_size = int(match.group(3))\n",
    "        else:\n",
    "            batch_size = -1\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            print(f\"dropping {df.isna().sum().sum()} NaNs\")\n",
    "            df.dropna(inplace=True)\n",
    "            df[\"context_size\"] = context_size\n",
    "            df[\"max_gen_len\"] = max_gen_len\n",
    "            df[\"batch_size\"] = batch_size\n",
    "            df[\"model\"] = re.search(model_regex, f).group(1)\n",
    "            df[\"device\"] = le_device\n",
    "            df[\"app\"] = application\n",
    "            dfs.append(df)\n",
    "        except EmptyDataError as e:\n",
    "            print(f\"Empty file: {f}\")\n",
    "            continue\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df[\"discharge_per_token\"] = df[\"discharge_pt (mAh)\"] / df[\"output_tokens\"]\n",
    "\n",
    "def filter_per_quantile(df, value, q):\n",
    "    r_quantile = df[value].quantile(q)\n",
    "    l_quantile = df[value].quantile(1-q)\n",
    "    # Filter values greater than the quantile value\n",
    "    filtered_df = df[(df[value] > l_quantile) & (df[value] < r_quantile)]\n",
    "    return filtered_df\n",
    "\n",
    "grouped_df_all = df.groupby([\"device\", \"model\", \"app\", \"iteration\"])\\\n",
    "                    .apply(filter_per_quantile, \"tps\", 0.95)\\\n",
    "                    .reset_index(drop=True)\\\n",
    "                    .groupby([\"device\", \"model\", \"app\"])\n",
    "grouped_df_tps = pd.concat([grouped_df_all.mean()[[\"tps\", \"prefill_tps\", \"input_tokens\", \"output_tokens\"]],\n",
    "                            grouped_df_all.std()[[\"tps\", \"prefill_tps\", \"input_tokens\", \"output_tokens\"]].rename(columns={\"tps\": \"std_tps\",\n",
    "                                                                                                                          \"prefill_tps\": \"std_prefill_tps\",\n",
    "                                                                                                                          \"input_tokens\": \"std_input_tokens\",\n",
    "                                                                                                                          \"output_tokens\": \"std_output_tokens\"})],\n",
    "                            axis=1)\n",
    "\n",
    "\n",
    "# grouped_df_all_energy = df.groupby([\"device\", \"model\", \"app\", \"iteration\"]).sum()[['energy_pt (mWh)', 'discharge_pt (mAh)', 'discharge_per_token']]\n",
    "grouped_df_all_energy = df.groupby([\"device\", \"model\", \"app\", \"iteration\"])\\\n",
    "                            .apply(filter_per_quantile, \"discharge_per_token\", 0.95)\\\n",
    "                            .reset_index(drop=True)\\\n",
    "                            .groupby([\"device\", \"model\", \"app\", \"iteration\"])\\\n",
    "                            .sum()[['energy_pt (mWh)', 'discharge_pt (mAh)', 'discharge_per_token']]\n",
    "# display(grouped_df_all_energy)\n",
    "\n",
    "grouped_df_energy = grouped_df_all_energy.reset_index().groupby([\"device\", \"model\", \"app\"]).mean()[['energy_pt (mWh)', 'discharge_pt (mAh)', 'discharge_per_token']]\n",
    "grouped_df_energy = pd.concat([grouped_df_energy,\n",
    "                               grouped_df_all_energy.reset_index().groupby([\"device\", \"model\", \"app\"])[[\"energy_pt (mWh)\", 'discharge_pt (mAh)', 'discharge_per_token']].std().rename(columns={\"energy_pt (mWh)\": \"std_energy_pt (mWh)\",\n",
    "                                                                                                                                                                                               \"discharge_pt (mAh)\": \"std_discharge_pt (mAh)\",\n",
    "                                                                                                                                                                                               \"discharge_per_token\": \"std_discharge_per_token\"})],\n",
    "                              axis=1)\n",
    "\n",
    "grouped_df_energy.reset_index(inplace=True)\n",
    "grouped_df_tps.reset_index(inplace=True)\n",
    "display(grouped_df_tps)\n",
    "display(grouped_df_energy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
